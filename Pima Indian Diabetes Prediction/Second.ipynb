{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy import stats\r\n",
    "import math\r\n",
    "\r\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\r\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.preprocessing import Normalizer\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.experimental import enable_iterative_imputer\r\n",
    "from sklearn.impute import IterativeImputer\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>BMI</th>\n",
       "      <th>F2</th>\n",
       "      <th>F6</th>\n",
       "      <th>F8</th>\n",
       "      <th>F12</th>\n",
       "      <th>F14</th>\n",
       "      <th>F20</th>\n",
       "      <th>F26</th>\n",
       "      <th>...</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>N11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865673</td>\n",
       "      <td>-0.029828</td>\n",
       "      <td>0.167923</td>\n",
       "      <td>0.114266</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>-0.234291</td>\n",
       "      <td>-0.306076</td>\n",
       "      <td>-0.108757</td>\n",
       "      <td>-0.218354</td>\n",
       "      <td>-0.561273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>-1.861482</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>-1.031754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.203506</td>\n",
       "      <td>-0.524085</td>\n",
       "      <td>-0.850370</td>\n",
       "      <td>-0.926240</td>\n",
       "      <td>-1.014180</td>\n",
       "      <td>-0.478410</td>\n",
       "      <td>-0.775792</td>\n",
       "      <td>-1.379022</td>\n",
       "      <td>-0.882523</td>\n",
       "      <td>-0.537655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>1.269742</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>1.534181</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>2.268039</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>0.969223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.015217</td>\n",
       "      <td>-0.688837</td>\n",
       "      <td>-1.330422</td>\n",
       "      <td>0.229082</td>\n",
       "      <td>1.704323</td>\n",
       "      <td>-0.179526</td>\n",
       "      <td>-0.142847</td>\n",
       "      <td>1.773290</td>\n",
       "      <td>2.020562</td>\n",
       "      <td>-0.421290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>1.269742</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>-1.861482</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>-1.031754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.072129</td>\n",
       "      <td>-0.524085</td>\n",
       "      <td>-0.632164</td>\n",
       "      <td>-0.945359</td>\n",
       "      <td>-0.942589</td>\n",
       "      <td>-0.539075</td>\n",
       "      <td>-0.556392</td>\n",
       "      <td>-1.156637</td>\n",
       "      <td>-0.402118</td>\n",
       "      <td>0.584352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358103</td>\n",
       "      <td>1.269742</td>\n",
       "      <td>0.931994</td>\n",
       "      <td>1.534181</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>2.268039</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>0.969223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.504388</td>\n",
       "      <td>-2.665865</td>\n",
       "      <td>1.549893</td>\n",
       "      <td>-0.235534</td>\n",
       "      <td>-0.327062</td>\n",
       "      <td>-0.272491</td>\n",
       "      <td>-0.845741</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>-0.379227</td>\n",
       "      <td>-1.241059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>-1.031754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>-0.678000</td>\n",
       "      <td>0.299676</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>2.228462</td>\n",
       "      <td>0.369406</td>\n",
       "      <td>0.284878</td>\n",
       "      <td>3.112461</td>\n",
       "      <td>0.451950</td>\n",
       "      <td>-1.305771</td>\n",
       "      <td>0.807038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>1.534181</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>-1.031754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.011726</td>\n",
       "      <td>-0.194580</td>\n",
       "      <td>0.633429</td>\n",
       "      <td>-0.758870</td>\n",
       "      <td>-0.699093</td>\n",
       "      <td>-0.505931</td>\n",
       "      <td>-0.563778</td>\n",
       "      <td>-0.773640</td>\n",
       "      <td>-0.069939</td>\n",
       "      <td>-0.087499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>0.931994</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>0.969223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.021118</td>\n",
       "      <td>-0.029828</td>\n",
       "      <td>-0.908558</td>\n",
       "      <td>0.142534</td>\n",
       "      <td>0.441493</td>\n",
       "      <td>0.084697</td>\n",
       "      <td>0.451492</td>\n",
       "      <td>0.566849</td>\n",
       "      <td>0.310048</td>\n",
       "      <td>0.433642</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>1.269742</td>\n",
       "      <td>0.931994</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>-1.031754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.143103</td>\n",
       "      <td>-1.018342</td>\n",
       "      <td>-0.341223</td>\n",
       "      <td>-1.064664</td>\n",
       "      <td>-1.006409</td>\n",
       "      <td>-0.631185</td>\n",
       "      <td>-0.774651</td>\n",
       "      <td>-1.538004</td>\n",
       "      <td>-0.095872</td>\n",
       "      <td>-0.078979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736321</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>-1.072968</td>\n",
       "      <td>-0.651814</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>0.969223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>-0.940753</td>\n",
       "      <td>-0.194580</td>\n",
       "      <td>-0.297582</td>\n",
       "      <td>-0.962834</td>\n",
       "      <td>-1.031885</td>\n",
       "      <td>-0.526238</td>\n",
       "      <td>-0.753034</td>\n",
       "      <td>-1.216584</td>\n",
       "      <td>-0.847222</td>\n",
       "      <td>-0.332860</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358103</td>\n",
       "      <td>-0.787562</td>\n",
       "      <td>0.931994</td>\n",
       "      <td>1.534181</td>\n",
       "      <td>-0.486534</td>\n",
       "      <td>-0.430331</td>\n",
       "      <td>-0.440910</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.609449</td>\n",
       "      <td>0.969223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Glucose  BloodPressure       BMI        F2        F6        F8  \\\n",
       "0    0.865673      -0.029828  0.167923  0.114266  0.075700 -0.234291   \n",
       "1   -1.203506      -0.524085 -0.850370 -0.926240 -1.014180 -0.478410   \n",
       "2    2.015217      -0.688837 -1.330422  0.229082  1.704323 -0.179526   \n",
       "3   -1.072129      -0.524085 -0.632164 -0.945359 -0.942589 -0.539075   \n",
       "4    0.504388      -2.665865  1.549893 -0.235534 -0.327062 -0.272491   \n",
       "..        ...            ...       ...       ...       ...       ...   \n",
       "763 -0.678000       0.299676  0.066094  2.228462  0.369406  0.284878   \n",
       "764  0.011726      -0.194580  0.633429 -0.758870 -0.699093 -0.505931   \n",
       "765 -0.021118      -0.029828 -0.908558  0.142534  0.441493  0.084697   \n",
       "766  0.143103      -1.018342 -0.341223 -1.064664 -1.006409 -0.631185   \n",
       "767 -0.940753      -0.194580 -0.297582 -0.962834 -1.031885 -0.526238   \n",
       "\n",
       "          F12       F14       F20       F26  ...        N1        N2  \\\n",
       "0   -0.306076 -0.108757 -0.218354 -0.561273  ... -0.736321 -0.787562   \n",
       "1   -0.775792 -1.379022 -0.882523 -0.537655  ... -0.736321  1.269742   \n",
       "2   -0.142847  1.773290  2.020562 -0.421290  ... -0.736321  1.269742   \n",
       "3   -0.556392 -1.156637 -0.402118  0.584352  ...  1.358103  1.269742   \n",
       "4   -0.845741  0.009133 -0.379227 -1.241059  ... -0.736321 -0.787562   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "763  3.112461  0.451950 -1.305771  0.807038  ... -0.736321 -0.787562   \n",
       "764 -0.563778 -0.773640 -0.069939 -0.087499  ... -0.736321 -0.787562   \n",
       "765  0.451492  0.566849  0.310048  0.433642  ... -0.736321  1.269742   \n",
       "766 -0.774651 -1.538004 -0.095872 -0.078979  ... -0.736321 -0.787562   \n",
       "767 -0.753034 -1.216584 -0.847222 -0.332860  ...  1.358103 -0.787562   \n",
       "\n",
       "           N3        N4        N5        N6        N7        N9       N10  \\\n",
       "0   -1.072968 -0.651814 -0.486534 -0.430331 -0.440910 -1.861482  0.609449   \n",
       "1   -1.072968  1.534181 -0.486534 -0.430331  2.268039  0.537206  0.609449   \n",
       "2   -1.072968 -0.651814 -0.486534 -0.430331 -0.440910 -1.861482  0.609449   \n",
       "3    0.931994  1.534181 -0.486534 -0.430331  2.268039  0.537206  0.609449   \n",
       "4   -1.072968 -0.651814 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "763 -1.072968  1.534181 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "764  0.931994 -0.651814 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "765  0.931994 -0.651814 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "766 -1.072968 -0.651814 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "767  0.931994  1.534181 -0.486534 -0.430331 -0.440910  0.537206  0.609449   \n",
       "\n",
       "          N11  \n",
       "0   -1.031754  \n",
       "1    0.969223  \n",
       "2   -1.031754  \n",
       "3    0.969223  \n",
       "4   -1.031754  \n",
       "..        ...  \n",
       "763 -1.031754  \n",
       "764  0.969223  \n",
       "765 -1.031754  \n",
       "766  0.969223  \n",
       "767  0.969223  \n",
       "\n",
       "[768 rows x 45 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\r\n",
    "\r\n",
    "df[list(df.columns)[:-1]] = df[list(df.columns)[:-1]].replace(0, np.NaN)\r\n",
    "df[list(df.columns)[:-1]] = pd.DataFrame(IterativeImputer(max_iter=100, random_state=0)\r\n",
    "                                         .fit_transform(df[list(df.columns)[:-1]]), columns=df[list(df.columns)[:-1]].columns)\r\n",
    "\r\n",
    "index = 1\r\n",
    "features = list(df.columns)[:-1]\r\n",
    "for feature1 in features:\r\n",
    "    for feature2 in features:\r\n",
    "        if feature1 != feature2:\r\n",
    "            df[f'F{index}'] = df[feature1] * df[feature2]\r\n",
    "            df[f'F{index+1}'] = df[feature1] / df[feature2]\r\n",
    "            index += 2\r\n",
    "\r\n",
    "df.loc[:,'N1']=0\r\n",
    "df.loc[(df['Age']<=30) & (df['Glucose']<=120),'N1']=1\r\n",
    "df.loc[:,'N2']=0\r\n",
    "df.loc[(df['BMI']<=30),'N2']=1\r\n",
    "df.loc[:,'N3']=0\r\n",
    "df.loc[(df['Age']<=30) & (df['Pregnancies']<=6),'N3']=1\r\n",
    "df.loc[:,'N4']=0\r\n",
    "df.loc[(df['Glucose']<=105) & (df['BloodPressure']<=80),'N4']=1\r\n",
    "df.loc[:,'N5']=0\r\n",
    "df.loc[(df['SkinThickness']<=20) ,'N5']=1\r\n",
    "df.loc[:,'N6']=0\r\n",
    "df.loc[(df['BMI']<30) & (df['SkinThickness']<=20),'N6']=1\r\n",
    "df.loc[:,'N7']=0\r\n",
    "df.loc[(df['Glucose']<=105) & (df['BMI']<=30),'N7']=1\r\n",
    "df.loc[:,'N9']=0\r\n",
    "df.loc[(df['Insulin']<200),'N9']=1\r\n",
    "df.loc[:,'N10']=0\r\n",
    "df.loc[(df['BloodPressure']<80),'N10']=1\r\n",
    "df.loc[:,'N11']=0\r\n",
    "df.loc[(df['Pregnancies']<4) & (df['Pregnancies']!=0) ,'N11']=1\r\n",
    "    \r\n",
    "high_corr = df.corr()[abs(df.corr())>=.9]\r\n",
    "del_cols = []\r\n",
    "for row in range(len(high_corr)):\r\n",
    "    for col in range(len(high_corr)):\r\n",
    "        if row != col and not math.isnan(high_corr.iloc[row, col]):\r\n",
    "            del_cols.append(row)\r\n",
    "del_cols = list(set(del_cols))\r\n",
    "del_cols = [list(df.columns)[i] for i in del_cols]\r\n",
    "df = df.drop(columns=list(set(del_cols)))\r\n",
    "\r\n",
    "X = df.drop(columns=['Outcome'])\r\n",
    "y = df['Outcome']\r\n",
    "\r\n",
    "scaled = StandardScaler().fit_transform(X)\r\n",
    "X = pd.DataFrame(scaled,columns=X.columns)\r\n",
    "\r\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.95488721804512, 3.817417209455358)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\r\n",
    "\r\n",
    "# model = DecisionTreeClassifier(min_samples_split=2,\r\n",
    "#                                max_depth=4,\r\n",
    "#                                criterion='entropy')\r\n",
    "model = LogisticRegression(max_iter=1000,\r\n",
    "                           tol=1e-5, \r\n",
    "                           solver='saga')\r\n",
    "\r\n",
    "scores = []\r\n",
    "for train_index , test_index in kfold.split(X):\r\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\r\n",
    "    y_train , y_test = y[train_index] , y[test_index]\r\n",
    "    # X_train, y_train = SMOTE(random_state=0).fit_resample(X_train, y_train)\r\n",
    "    model.fit(X_train,y_train)\r\n",
    "    y_pred = model.predict(X_test)\r\n",
    "    # sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\r\n",
    "    # plt.show()\r\n",
    "    scores.append(accuracy_score(y_test, y_pred) * 100)\r\n",
    "\r\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.770830 (0.036047)\n",
      "LDA: 0.770830 (0.044130)\n",
      "KNN: 0.722608 (0.049676)\n",
      "CART: 0.705759 (0.050198)\n",
      "NB: 0.723975 (0.041889)\n",
      "SVM: 0.747403 (0.060734)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = []\r\n",
    "models.append(('LR', LogisticRegression(max_iter =1000)))\r\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\r\n",
    "models.append(('KNN', KNeighborsClassifier()))\r\n",
    "models.append(('CART', DecisionTreeClassifier()))\r\n",
    "models.append(('NB', GaussianNB()))\r\n",
    "models.append(('SVM', SVC(max_iter =1000)))\r\n",
    "\r\n",
    "# evaluate each model in turn\r\n",
    "results = []\r\n",
    "names = []\r\n",
    "scoring = 'accuracy'\r\n",
    "for name, model in models:\r\n",
    "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\r\n",
    "    results.append(cv_results)\r\n",
    "    names.append(name)\r\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\r\n",
    "    print(msg)\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "fig.suptitle('Algorithm Comparison')\r\n",
    "ax = fig.add_subplot(111)\r\n",
    "plt.boxplot(results)\r\n",
    "ax.set_xticklabels(names)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "285 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 434, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got netwon-cg.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rando\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tol': 1e-06,\n",
       " 'solver': 'sag',\n",
       " 'penalty': 'l2',\n",
       " 'class_weight': None,\n",
       " 'C': 0.01}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {'penalty': ['l1', 'l2', 'elasticnet'],\r\n",
    "         'tol': [1e-4, 1e-5, 1e-6],\r\n",
    "         'C': [0.01, 0.1, 1, 10],\r\n",
    "         'class_weight': [None, 'balanced'],\r\n",
    "         'solver': ['netwon-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\r\n",
    "\r\n",
    "clf = RandomizedSearchCV(LogisticRegression(random_state=0, max_iter=10000), param, n_iter=100, scoring='accuracy', n_jobs=-1, cv=5, random_state=0)\r\n",
    "search = clf.fit(X, y)\r\n",
    "search.best_params_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}